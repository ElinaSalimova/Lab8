---
title: "Упражнение №8"
author: "Салимова Элина"
date: '26 апреля 2018 г '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
library('tree')  
library('ISLR') 
library('randomForest') 
library('gbm')
```

# Деревья решений

Загрузим таблицу с данными по зарплатам и добавим к ней переменную High – “высокая зарплата” со значениями:

1 если зарплата больше 128.68;
0 в противном случае;

```{r, echo=TRUE}
attach(Wage)
# новая переменная
High <- ifelse(wage >= 128.68, '1', '0')
# присоединяем к таблице данных
Wage <- data.frame(Wage, High)
# модель бинарного  дерева
tree.wage <- tree(High ~ . -wage -region -logwage, Wage)
summary(tree.wage)

# график результата
plot(tree.wage)            # ветви
text(tree.wage, pretty=0)  # подписи

tree.wage                # посмотреть всё дерево в консоли

# ядро генератора случайных чисел
set.seed(6)
# обучающая выборка
train <- sample(1:nrow(Wage), 1500)
# тестовая выборка
Wage.test <- Wage[-train,]
High.test <- High[-train]

# строим дерево на обучающей выборке
tree.wage <- tree(High ~ . -wage -region -logwage, Wage, subset = train)

# делаем прогноз
tree.pred <- predict(tree.wage, Wage.test, type = "class")

# матрица неточностей
tbl <- table(tree.pred, High.test)
tbl

# оценка точности
acc.test <- sum(diag(tbl))/sum(tbl)
acc.test
```

Доля верных прогнозов: 0.774.

Теперь обрезаем дерево, используя в качестве критерия частоту ошибок классификации. Функция cv.tree() проводит кросс-валидацию для выбора лучшего дерева, аргумент prune.misclass означает, что мы минимизируем ошибку классификации.

```{r, echo=TRUE}
cv.wage <- cv.tree(tree.wage, FUN = prune.misclass)
# имена элементов полученного объекта
names(cv.wage)

cv.wage

# графики изменения параметров метода по ходу обрезки дерева ###################

# 1. ошибка с кросс-валидацией в зависимости от числа узлов
par(mfrow = c(1, 2))
plot(cv.wage$size, cv.wage$dev, type = "b",
     ylab = 'Частота ошибок с кросс-вал. (dev)',
     xlab = 'Число узлов (size)')
# размер дерева с минимальной ошибкой
opt.size <- cv.wage$size[cv.wage$dev == min(cv.wage$dev)]
abline(v = opt.size, col = 'red', 'lwd' = 2)     # соотв. вертикальная прямая
mtext(opt.size, at = opt.size, side = 1, col = 'red', line = 1)

# 2. ошибка с кросс-валидацией в зависимости от штрафа на сложность
plot(cv.wage$k, cv.wage$dev, type = "b",
     ylab = 'Частота ошибок с кросс-вал. (dev)',
     xlab = 'Штраф за сложность (k)')
```

Как видно на графике слева, минимум частоты ошибок достигается при числе узлов 3, 4 и 6. Выбираем минимальное значение.

```{r, echo=TRUE}
# дерево с 3 узлами
prune.wage <- prune.misclass(tree.wage, best = 3)

# визуализация
plot(prune.wage)
text(prune.wage, pretty = 0)

# прогноз на тестовую выборку
tree.pred <- predict(prune.wage, Wage.test, type = "class")

# матрица неточностей
tbl <- table(tree.pred, High.test)
tbl

# оценка точности
acc.test <- sum(diag(tbl))/sum(tbl)
acc.test

par(mfrow = c(1, 1))
```

# Бустинг

Реализуем бустинг для зависимой переменной wage. 

```{r, echo=TRUE}
# rf
wage.test <- Wage[-train, "wage"]
rf.wage <- randomForest(wage ~ . -High -region -logwage, data = Wage, subset = train,
                          mtry = 6, importance = TRUE)
# прогноз
yhat.rf <- predict(rf.wage, newdata = Wage[-train, ])
# MSE на тестовой выборке
mse.test <- mean((yhat.rf - wage.test)^2)
# важность предикторов
importance(rf.wage)  # оценки

boost.wage <- gbm(wage ~ . -High -region -logwage, data = Wage[train, ], distribution = "gaussian",
                    n.trees = 5000, interaction.depth = 4)
# график и таблица относительной важности переменных
summary(boost.wage)

# графики частной зависимости для двух наиболее важных предикторов
par(mfrow = c(1, 2))
plot(boost.wage, i = "maritl")
plot(boost.wage, i = "education")


# прогноз
yhat.boost <- predict(boost.wage, newdata = Wage[-train, ], n.trees = 5000)

# MSE на тестовой
mse.test <- mean((yhat.boost - wage.test)^2)
mse.test
```

Получим MSE, равную 1177.
Настройку бустинга можно делать с помощью гиперпараметра λ (аргумент shrinkage). Установим его равным 0.1.

```{r, echo=TRUE}
# меняем значение гиперпараметра (lambda) на 0.2 -- аргумент shrinkage
boost.wage <- gbm(wage ~ . -High -region -logwage, data = Wage[train, ], distribution = "gaussian",
                    n.trees = 5000, interaction.depth = 4, 
                    shrinkage = 0.1, verbose = F)
# прогноз
yhat.boost <- predict(boost.wage, newdata = Wage[-train, ], n.trees = 5000)

# MSE а тестовой
mse.test <- mean((yhat.boost - wage.test)^2)
mse.test
```

В данном случае MSE получилась равной 1623, что больше полученной ранее.